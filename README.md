# Codenames

I used Numpy and Re to implement Word2vec's CBOW model from scratch by processing a corpus and training a feed-forward neural network to generate word embedding vectors. 
Using the training embeddings, I returned the word whose embedding's Euclidean distance from the target word's embedding was minimum. 

This project was inspired by Stanford's CS224N Youtube Course and adapted from a Word2vec implementation by Jake Tae, found at https://jaketae.github.io/study/word2vec/.
